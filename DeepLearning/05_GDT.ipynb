{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Unita: GDT\n",
    "\n",
    "Semplice classificazione basata su Logistic Regression e Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Partiamo dall'output folder dell'unità 04_notMNIST specificando dove e' posizionato il file pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = \"DeepLearning/data/notMNIST.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\nValidation set (10000, 28, 28) (10000,)\nTest set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# carico i dati\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  saved = pickle.load(f)\n",
    "  train_dataset = saved['train_dataset']\n",
    "  train_labels = saved['train_labels']\n",
    "  valid_dataset = saved['valid_dataset']\n",
    "  valid_labels = saved['valid_labels']\n",
    "  test_dataset = saved['test_dataset']\n",
    "  test_labels = saved['test_labels']\n",
    "  del saved  # garbage collector per liberare memoria\n",
    "  \n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.02156863  0.17058824  0.1627451   0.16666667\n   0.17450981  0.18235295  0.18627451  0.19411765  0.19803922  0.20588236\n   0.20980392  0.21372549  0.22156863  0.22941177  0.22941177  0.2372549\n   0.24117647  0.25686276 -0.00196078]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5         0.28431374  0.5         0.5         0.5         0.5\n   0.5         0.5         0.5         0.5         0.5         0.5         0.5\n   0.5         0.5         0.5         0.5         0.49607843  0.5\n   0.21764706]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5         0.32745099  0.5         0.5         0.5         0.5\n   0.5         0.5         0.5         0.49607843  0.48823529  0.48823529\n   0.48823529  0.48823529  0.49607843  0.5         0.5         0.5         0.5\n   0.30784315]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.14705883 -0.04901961 -0.04901961 -0.0372549\n  -0.02941176 -0.02156863 -0.00588235 -0.03333334  0.39411765  0.5         0.5\n   0.5         0.49607843  0.5         0.19019608 -0.0254902   0.02941176\n   0.03333334 -0.07254902]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.48431373 -0.5         0.28823531  0.5         0.49607843\n   0.49607843  0.49607843  0.5        -0.12352941 -0.5        -0.48039216\n  -0.5        -0.5       ]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.48823529 -0.48431373 -0.48431373 -0.48431373\n  -0.48431373 -0.48431373 -0.46862745 -0.5         0.31176472  0.5\n   0.49607843  0.49607843  0.49215686  0.5        -0.0882353  -0.5\n  -0.46470588 -0.48431373 -0.48823529]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.48431373 -0.5         0.31960785  0.5         0.49607843\n   0.49607843  0.48823529  0.5        -0.08431373 -0.5        -0.48039216\n  -0.5        -0.5       ]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.48431373 -0.5         0.33137256  0.5         0.49607843\n   0.5         0.48431373  0.5        -0.06862745 -0.5        -0.47647059\n  -0.5        -0.5       ]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.48431373 -0.5         0.34313726  0.5         0.49607843\n   0.5         0.48039216  0.5        -0.05294118 -0.5        -0.47647059\n  -0.5        -0.5       ]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.48431373 -0.5         0.35882354  0.5         0.49607843\n   0.5         0.48039216  0.5        -0.04117647 -0.5        -0.47647059\n  -0.5        -0.5       ]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.48431373 -0.5         0.3509804   0.5         0.49607843\n   0.5         0.47647059  0.5        -0.02156863 -0.5        -0.47647059\n  -0.5        -0.5       ]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.48431373 -0.5         0.28823531  0.5         0.49607843\n   0.5         0.47647059  0.5        -0.01764706 -0.5        -0.47647059\n  -0.5        -0.5       ]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.48431373 -0.5         0.22156863  0.5         0.5         0.5\n   0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5       ]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.48823529 -0.5         0.15490197  0.5         0.5         0.5\n   0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5       ]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.48823529 -0.5         0.09215686  0.5         0.5         0.5\n   0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5       ]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.48823529 -0.5         0.0254902   0.5         0.5         0.5\n   0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5       ]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.49215686 -0.5        -0.04117647  0.49607843  0.5         0.5\n   0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5       ]\n [-0.49607843 -0.48823529 -0.48823529 -0.48431373 -0.48431373 -0.48431373\n  -0.48431373 -0.49607843 -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.49215686 -0.5        -0.10784314  0.48823529\n   0.5         0.5         0.47647059  0.5        -0.01764706 -0.5\n  -0.47647059 -0.5        -0.5       ]\n [-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  -0.5        -0.49215686 -0.5        -0.17450981  0.48039216  0.5         0.5\n   0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5       ]\n [-0.40196079 -0.11960784 -0.11960784 -0.10784314 -0.09607843 -0.09607843\n  -0.0372549  -0.4137255  -0.5        -0.49607843 -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.49607843 -0.5        -0.24117647  0.47254902\n   0.5         0.5         0.47647059  0.5        -0.01764706 -0.5\n  -0.47647059 -0.5        -0.5       ]\n [-0.24901961  0.5         0.49215686  0.5         0.5         0.47647059\n   0.5        -0.30000001 -0.5        -0.49215686 -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.49607843 -0.5        -0.30784315  0.46862745\n   0.5         0.5         0.47647059  0.5        -0.01764706 -0.5\n  -0.47647059 -0.5        -0.5       ]\n [-0.30000001  0.5         0.48039216  0.48823529  0.48823529  0.46470588\n   0.5        -0.30000001 -0.5        -0.49215686 -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.49607843 -0.5        -0.37058824  0.46078432\n   0.5         0.5         0.47647059  0.5        -0.01764706 -0.5\n  -0.47647059 -0.5        -0.5       ]\n [-0.33137256  0.5         0.49215686  0.5         0.5         0.47647059\n   0.5        -0.30000001 -0.5        -0.47647059 -0.48431373 -0.48431373\n  -0.48431373 -0.48431373 -0.48431373 -0.48039216 -0.49215686 -0.46862745\n   0.44901961  0.5         0.5         0.47647059  0.5        -0.02156863\n  -0.5        -0.47647059 -0.5        -0.5       ]\n [-0.37058824  0.5         0.48039216  0.5         0.5         0.47647059\n   0.5        -0.28039217 -0.5        -0.49215686 -0.5        -0.5        -0.5\n  -0.5        -0.5        -0.49607843 -0.5        -0.37058824  0.46078432\n   0.5         0.49607843  0.46470588  0.5        -0.01764706 -0.5\n  -0.47647059 -0.5        -0.5       ]\n [-0.41764706  0.44901961  0.5         0.49215686  0.49215686  0.49607843\n   0.5         0.40588236  0.18627451  0.00980392  0.00588235  0.00980392\n   0.01372549  0.02156863  0.02941176  0.02156863  0.20196079  0.45294118\n   0.5         0.49607843  0.48431373  0.5         0.5        -0.13137256\n  -0.5        -0.48039216 -0.5        -0.5       ]\n [-0.5        -0.37058824  0.15882353  0.49607843  0.49607843  0.49215686\n   0.48431373  0.49607843  0.5         0.5         0.5         0.5         0.5\n   0.5         0.5         0.5         0.5         0.49607843  0.48823529\n   0.5         0.5         0.28431374 -0.28039217 -0.5        -0.49215686\n  -0.5        -0.5        -0.5       ]\n [-0.5        -0.5        -0.49607843 -0.24117647  0.28823531  0.5\n   0.49607843  0.5         0.5         0.5         0.5         0.5         0.5\n   0.5         0.5         0.5         0.5         0.5         0.5\n   0.4254902  -0.07647059 -0.48039216 -0.49607843 -0.49607843 -0.5        -0.5\n  -0.5        -0.5       ]\n [-0.5        -0.49215686 -0.48431373 -0.5        -0.46078432 -0.04509804\n   0.2254902   0.20588236  0.20588236  0.19803922  0.19411765  0.19019608\n   0.18627451  0.18235295  0.17450981  0.16666667  0.1627451   0.16666667\n   0.1        -0.3509804  -0.5        -0.49215686 -0.48823529 -0.5        -0.5\n  -0.5        -0.5        -0.5       ]]\n()\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "print(train_labels[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\nValidation set (10000, 784) (10000, 10)\nTest set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Riportiamo i dati nel formato adatto al processamento: matrix 1-dim + vettore 1-hot encoding\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  # -1 indica che la dimensione iniziale rimane invariata\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  # aggiungo una dimensione a labels\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.02156863  0.17058824  0.1627451   0.16666667\n  0.17450981  0.18235295  0.18627451  0.19411765  0.19803922  0.20588236\n  0.20980392  0.21372549  0.22156863  0.22941177  0.22941177  0.2372549\n  0.24117647  0.25686276 -0.00196078 -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5         0.28431374\n  0.5         0.5         0.5         0.5         0.5         0.5         0.5\n  0.5         0.5         0.5         0.5         0.5         0.5         0.5\n  0.5         0.49607843  0.5         0.21764706 -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n  0.32745099  0.5         0.5         0.5         0.5         0.5         0.5\n  0.5         0.49607843  0.48823529  0.48823529  0.48823529  0.48823529\n  0.49607843  0.5         0.5         0.5         0.5         0.30784315\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.14705883 -0.04901961 -0.04901961 -0.0372549\n -0.02941176 -0.02156863 -0.00588235 -0.03333334  0.39411765  0.5         0.5\n  0.5         0.49607843  0.5         0.19019608 -0.0254902   0.02941176\n  0.03333334 -0.07254902 -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.48431373 -0.5         0.28823531\n  0.5         0.49607843  0.49607843  0.49607843  0.5        -0.12352941\n -0.5        -0.48039216 -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.48823529 -0.48431373 -0.48431373 -0.48431373 -0.48431373 -0.48431373\n -0.46862745 -0.5         0.31176472  0.5         0.49607843  0.49607843\n  0.49215686  0.5        -0.0882353  -0.5        -0.46470588 -0.48431373\n -0.48823529 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.48431373 -0.5         0.31960785  0.5\n  0.49607843  0.49607843  0.48823529  0.5        -0.08431373 -0.5\n -0.48039216 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.48431373 -0.5\n  0.33137256  0.5         0.49607843  0.5         0.48431373  0.5\n -0.06862745 -0.5        -0.47647059 -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.48431373 -0.5         0.34313726  0.5         0.49607843  0.5\n  0.48039216  0.5        -0.05294118 -0.5        -0.47647059 -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.48431373 -0.5         0.35882354  0.5         0.49607843\n  0.5         0.48039216  0.5        -0.04117647 -0.5        -0.47647059\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.48431373 -0.5         0.3509804\n  0.5         0.49607843  0.5         0.47647059  0.5        -0.02156863\n -0.5        -0.47647059 -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.48431373\n -0.5         0.28823531  0.5         0.49607843  0.5         0.47647059\n  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.48431373 -0.5         0.22156863  0.5         0.5         0.5\n  0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.48823529 -0.5         0.15490197  0.5         0.5         0.5\n  0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.48823529 -0.5         0.09215686  0.5         0.5         0.5\n  0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.48823529 -0.5         0.0254902   0.5         0.5         0.5\n  0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.49215686 -0.5        -0.04117647  0.49607843  0.5         0.5\n  0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5\n -0.49607843 -0.48823529 -0.48823529 -0.48431373 -0.48431373 -0.48431373\n -0.48431373 -0.49607843 -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.49215686 -0.5        -0.10784314  0.48823529\n  0.5         0.5         0.47647059  0.5        -0.01764706 -0.5\n -0.47647059 -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.5        -0.5        -0.5\n -0.5        -0.5        -0.5        -0.5        -0.49215686 -0.5\n -0.17450981  0.48039216  0.5         0.5         0.47647059  0.5\n -0.01764706 -0.5        -0.47647059 -0.5        -0.5        -0.40196079\n -0.11960784 -0.11960784 -0.10784314 -0.09607843 -0.09607843 -0.0372549\n -0.4137255  -0.5        -0.49607843 -0.5        -0.5        -0.5        -0.5\n -0.5        -0.49607843 -0.5        -0.24117647  0.47254902  0.5         0.5\n  0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5\n -0.24901961  0.5         0.49215686  0.5         0.5         0.47647059\n  0.5        -0.30000001 -0.5        -0.49215686 -0.5        -0.5        -0.5\n -0.5        -0.5        -0.49607843 -0.5        -0.30784315  0.46862745\n  0.5         0.5         0.47647059  0.5        -0.01764706 -0.5\n -0.47647059 -0.5        -0.5        -0.30000001  0.5         0.48039216\n  0.48823529  0.48823529  0.46470588  0.5        -0.30000001 -0.5\n -0.49215686 -0.5        -0.5        -0.5        -0.5        -0.5\n -0.49607843 -0.5        -0.37058824  0.46078432  0.5         0.5\n  0.47647059  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5\n -0.33137256  0.5         0.49215686  0.5         0.5         0.47647059\n  0.5        -0.30000001 -0.5        -0.47647059 -0.48431373 -0.48431373\n -0.48431373 -0.48431373 -0.48431373 -0.48039216 -0.49215686 -0.46862745\n  0.44901961  0.5         0.5         0.47647059  0.5        -0.02156863\n -0.5        -0.47647059 -0.5        -0.5        -0.37058824  0.5\n  0.48039216  0.5         0.5         0.47647059  0.5        -0.28039217\n -0.5        -0.49215686 -0.5        -0.5        -0.5        -0.5        -0.5\n -0.49607843 -0.5        -0.37058824  0.46078432  0.5         0.49607843\n  0.46470588  0.5        -0.01764706 -0.5        -0.47647059 -0.5        -0.5\n -0.41764706  0.44901961  0.5         0.49215686  0.49215686  0.49607843\n  0.5         0.40588236  0.18627451  0.00980392  0.00588235  0.00980392\n  0.01372549  0.02156863  0.02941176  0.02156863  0.20196079  0.45294118\n  0.5         0.49607843  0.48431373  0.5         0.5        -0.13137256\n -0.5        -0.48039216 -0.5        -0.5        -0.5        -0.37058824\n  0.15882353  0.49607843  0.49607843  0.49215686  0.48431373  0.49607843\n  0.5         0.5         0.5         0.5         0.5         0.5         0.5\n  0.5         0.5         0.49607843  0.48823529  0.5         0.5\n  0.28431374 -0.28039217 -0.5        -0.49215686 -0.5        -0.5        -0.5\n -0.5        -0.5        -0.49607843 -0.24117647  0.28823531  0.5\n  0.49607843  0.5         0.5         0.5         0.5         0.5         0.5\n  0.5         0.5         0.5         0.5         0.5         0.5\n  0.4254902  -0.07647059 -0.48039216 -0.49607843 -0.49607843 -0.5        -0.5\n -0.5        -0.5        -0.5        -0.49215686 -0.48431373 -0.5\n -0.46078432 -0.04509804  0.2254902   0.20588236  0.20588236  0.19803922\n  0.19411765  0.19019608  0.18627451  0.18235295  0.17450981  0.16666667\n  0.1627451   0.16666667  0.1        -0.3509804  -0.5        -0.49215686\n -0.48823529 -0.5        -0.5        -0.5        -0.5        -0.5       ]\n(10,)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "print(train_labels[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## PROBLEMA #1\n",
    "\n",
    "Implementare una logistic regression multinomiale con discesa del gradiente con Tensorflow (TF) \n",
    "come classificatore per notMNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matrice dei pensi W viene spesso inizializzata con una variabile casuale con distribuzione normale,\n",
    "dove i valori maggiori di 2 x std_dev sono rimossi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([     0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,   2067.,   3007.,\n          3463.,   4080.,   4571.,   5182.,   5923.,   6924.,   7495.,\n          8371.,   9152.,  10146.,  10889.,  11724.,  12775.,  13419.,\n         14467.,  15088.,  15929.,  16350.,  16760.,  17140.,  17351.,\n         17841.,  17633.,  17573.,  16917.,  16777.,  16161.,  15784.,\n         15125.,  14319.,  13644.,  12665.,  11911.,  11088.,  10090.,\n          9225.,   8395.,   7388.,   6877.,   6025.,   5225.,   4600.,\n          3956.,   3469.,   2994.,   2045.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.]),\n array([-4.2  , -4.116, -4.032, -3.948, -3.864, -3.78 , -3.696, -3.612,\n        -3.528, -3.444, -3.36 , -3.276, -3.192, -3.108, -3.024, -2.94 ,\n        -2.856, -2.772, -2.688, -2.604, -2.52 , -2.436, -2.352, -2.268,\n        -2.184, -2.1  , -2.016, -1.932, -1.848, -1.764, -1.68 , -1.596,\n        -1.512, -1.428, -1.344, -1.26 , -1.176, -1.092, -1.008, -0.924,\n        -0.84 , -0.756, -0.672, -0.588, -0.504, -0.42 , -0.336, -0.252,\n        -0.168, -0.084,  0.   ,  0.084,  0.168,  0.252,  0.336,  0.42 ,\n         0.504,  0.588,  0.672,  0.756,  0.84 ,  0.924,  1.008,  1.092,\n         1.176,  1.26 ,  1.344,  1.428,  1.512,  1.596,  1.68 ,  1.764,\n         1.848,  1.932,  2.016,  2.1  ,  2.184,  2.268,  2.352,  2.436,\n         2.52 ,  2.604,  2.688,  2.772,  2.856,  2.94 ,  3.024,  3.108,\n         3.192,  3.276,  3.36 ,  3.444,  3.528,  3.612,  3.696,  3.78 ,\n         3.864,  3.948,  4.032,  4.116,  4.2  ]),\n <a list of 100 Patch objects>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ce4dc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# weights = tf.Variable( tf.truncated_normal(...))\n",
    "\n",
    "# Questo permette di ignorare valori troppo grandi o piccoli che possono influenzare negativamente l'apprendimento.\n",
    "\n",
    "n = 500000\n",
    "A = tf.truncated_normal((n,))\n",
    "B = tf.random_normal((n,))\n",
    "with tf.Session() as sess:\n",
    "    a, b = sess.run([A, B])\n",
    "\n",
    "pl.hist(a, 100, (-4.2, 4.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  6.00000000e+00,   4.00000000e+00,   9.00000000e+00,\n          1.00000000e+01,   1.30000000e+01,   2.50000000e+01,\n          2.00000000e+01,   2.60000000e+01,   3.70000000e+01,\n          4.60000000e+01,   6.30000000e+01,   1.01000000e+02,\n          1.03000000e+02,   1.39000000e+02,   2.00000000e+02,\n          2.52000000e+02,   3.33000000e+02,   4.03000000e+02,\n          5.07000000e+02,   6.56000000e+02,   7.66000000e+02,\n          9.17000000e+02,   1.19200000e+03,   1.38100000e+03,\n          1.64400000e+03,   1.95300000e+03,   2.39200000e+03,\n          2.82500000e+03,   3.30200000e+03,   3.81000000e+03,\n          4.34000000e+03,   4.92700000e+03,   5.68000000e+03,\n          6.46700000e+03,   7.10100000e+03,   8.00200000e+03,\n          8.75000000e+03,   9.55100000e+03,   1.04740000e+04,\n          1.15010000e+04,   1.21800000e+04,   1.28530000e+04,\n          1.37720000e+04,   1.45120000e+04,   1.51930000e+04,\n          1.57310000e+04,   1.60560000e+04,   1.62740000e+04,\n          1.67510000e+04,   1.66040000e+04,   1.67460000e+04,\n          1.67330000e+04,   1.64770000e+04,   1.60370000e+04,\n          1.57430000e+04,   1.50130000e+04,   1.42410000e+04,\n          1.38400000e+04,   1.29820000e+04,   1.20110000e+04,\n          1.11550000e+04,   1.06460000e+04,   9.64300000e+03,\n          8.65600000e+03,   8.03200000e+03,   7.31300000e+03,\n          6.56300000e+03,   5.63200000e+03,   4.99500000e+03,\n          4.40100000e+03,   3.83800000e+03,   3.27700000e+03,\n          2.81100000e+03,   2.32500000e+03,   1.97200000e+03,\n          1.70400000e+03,   1.38800000e+03,   1.19600000e+03,\n          9.76000000e+02,   8.18000000e+02,   6.13000000e+02,\n          5.03000000e+02,   4.39000000e+02,   3.29000000e+02,\n          2.61000000e+02,   2.00000000e+02,   1.64000000e+02,\n          1.13000000e+02,   8.50000000e+01,   7.50000000e+01,\n          5.60000000e+01,   3.60000000e+01,   2.80000000e+01,\n          1.70000000e+01,   2.20000000e+01,   1.00000000e+01,\n          7.00000000e+00,   8.00000000e+00,   5.00000000e+00,\n          3.00000000e+00]),\n array([-4.2  , -4.116, -4.032, -3.948, -3.864, -3.78 , -3.696, -3.612,\n        -3.528, -3.444, -3.36 , -3.276, -3.192, -3.108, -3.024, -2.94 ,\n        -2.856, -2.772, -2.688, -2.604, -2.52 , -2.436, -2.352, -2.268,\n        -2.184, -2.1  , -2.016, -1.932, -1.848, -1.764, -1.68 , -1.596,\n        -1.512, -1.428, -1.344, -1.26 , -1.176, -1.092, -1.008, -0.924,\n        -0.84 , -0.756, -0.672, -0.588, -0.504, -0.42 , -0.336, -0.252,\n        -0.168, -0.084,  0.   ,  0.084,  0.168,  0.252,  0.336,  0.42 ,\n         0.504,  0.588,  0.672,  0.756,  0.84 ,  0.924,  1.008,  1.092,\n         1.176,  1.26 ,  1.344,  1.428,  1.512,  1.596,  1.68 ,  1.764,\n         1.848,  1.932,  2.016,  2.1  ,  2.184,  2.268,  2.352,  2.436,\n         2.52 ,  2.604,  2.688,  2.772,  2.856,  2.94 ,  3.024,  3.108,\n         3.192,  3.276,  3.36 ,  3.444,  3.528,  3.612,  3.696,  3.78 ,\n         3.864,  3.948,  4.032,  4.116,  4.2  ]),\n <a list of 100 Patch objects>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ce16110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl.hist(b, 100, (-4.2, 4.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prendiamo un subset per limitare il tempo per l'addetramento, diminuiscilo se occorre\n",
    "\n",
    "train_subset = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Su Tensorflow ogni elemento - input, variabili ed elaborazioni - è descritto mediante un grafo, o dataflow graph. Gli oggetti tf.Operation rappresentano unità di computazione;\n",
    "\n",
    "Gli oggetti tf.Tensor rappresentano unità di dati (tensori) che sono usati come input e output per gli oggetti Operation.\n",
    "\n",
    "In TF un grafo tf.Graph contiene due tipi di informazione:\n",
    "\n",
    "· La struttura: nodi e archi che rappresentano le operazioni \n",
    "\n",
    "· Le collections: insiemi di metadati (inseriti con tf.add_to_collection) nella forma <chiave,lista di objects); si può ispezionare con tf.get_collection.\n",
    "\n",
    "TF usa questa struttura per salvare variabili e altre informazioni del grafo.\n",
    "\n",
    "Un oggetto Graph di default è sempre prensente e accedibile chiamando tf.get_default_graph. \n",
    "\n",
    "Un approccio alternativo per usare i grafo di Tensorflow consiste nel context manager tf.Graph.as_default, che sostituisce il grafo di default per tutta l'esistenza del contesto in esame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "# Costruisco un grafo di computazione con Tensorflow\n",
    "with graph.as_default():\n",
    "\n",
    "  # Creo tensori costanti per i seguenti set: trainig, test e validation\n",
    "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "  # per assegnare un nome alla variabile possiamo usare il secondo parametro, e.g., tf.constant(0, name=\"c\") \n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Le variabili mantengono lo stato durante le elaborazioni. Sono anch'esse tensori.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  # Il vettore di bias b e' inizializzato a 0.\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Calcolo Wx + b\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "\n",
    "  # La funzione softmax_cross_entropy_with_logits valuta la funzione di loss\n",
    "  # per mezzo della cross-entropy loss con l'output corretto (tf_train_labels)\n",
    "  # Mentre reduce_mean valuta semplicemente la media dei valori del tensore.\n",
    "  # loss indica una operazione TF.\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Instanzio un algoritmo di discesa del gradiente con learning rate = 0.5 (alfa nelle slide di richiami sulle reti neurali.)\n",
    "  # La funzione minimize e' composta di 2 elaborazioni: compute_gradients e apply_gradients.\n",
    "  # La prima ricava i gradienti, la seconda aggiorna la matrice dei pesi di conseguenza.\n",
    "  # optimizer indica una operazione TF.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "  # calcolo softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), dim) per i sets: \n",
    "  # training, validation e test.\n",
    "  # N.B.: i set valid e test sono usati solo per la valutazione, non c'e' backprop\n",
    "  # N.B.(2): ci servono per valutare l'accuratezza, l'apprendimento l'abbiamo gia' fatto.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  logits = tf.matmul(tf_valid_dataset, weights) + biases\n",
    "  \n",
    "  valid_prediction = tf.nn.softmax(logits) \n",
    "  logits = tf.matmul(tf_valid_dataset, weights) + biases\n",
    "                     \n",
    "  test_prediction = tf.nn.softmax(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numero cicli di elaborazione\n",
    "num_steps = 801"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definisco l'accuratezza come somma del numero di predizioni corrette normalizzato sul numero di predizioni totali.\n",
    "\n",
    "La uso per fare statistiche durante il funzionamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0: 16.904888\nTraining accuracy: 12.6%\nValidation accuracy: 15.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 100: 2.367931\nTraining accuracy: 71.1%\nValidation accuracy: 70.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 200: 1.895879\nTraining accuracy: 74.2%\nValidation accuracy: 73.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 300: 1.631782\nTraining accuracy: 75.7%\nValidation accuracy: 73.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 400: 1.454509\nTraining accuracy: 76.6%\nValidation accuracy: 74.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 500: 1.324679\nTraining accuracy: 77.3%\nValidation accuracy: 74.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 600: 1.224638\nTraining accuracy: 77.8%\nValidation accuracy: 74.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 700: 1.144739\nTraining accuracy: 78.1%\nValidation accuracy: 75.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 800: 1.079199\nTraining accuracy: 78.7%\nValidation accuracy: 75.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 10.1%\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "# TF usa tf.Session per rappresentare una connesione tra il programa e il runtime C++.\n",
    "# Serve per creare un ambiente in cui lanciare le operazioni definite nel grafo.\n",
    "# Poiche' la classe alloca risorse fisiche, solitamente si usa come context manager (dentro un blocco with),\n",
    "# che le libera automaticamente al termine del blocco, cioe' lancia session.close() al termine della esecuzione.\n",
    "with tf.Session(graph=graph) as session:\n",
    "  \n",
    "  # Istanzia e lancia una operazione per l'inizializzazione delle variabili globali del grafo\n",
    "  # cioe': weights e biases. Va eseguita solo una volta.\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "      \n",
    "  for step in range(num_steps):\n",
    "    \n",
    "    # Eseguo le operazioni nel grafo.\n",
    "    # Le operazioni e i tensori da valutare sono definiti nel primo parametro, un NumPy array.\n",
    "    # La lista indica le foglie grafo.\n",
    "    # Il valore di ritorno ha lo stesso tipo dell'input, cioe' un array, \n",
    "    # dove le foglie sono sostituite con il corrispondente valore calcolato da TF.\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    \n",
    "    # ogni tanto stampo statistiche\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(\n",
    "        predictions, train_labels[:train_subset, :]))\n",
    "      \n",
    "      # Se invoco eval() su valid_prediction, sto calcolando l'operazione sui \n",
    "      # pesi e bias correnti.\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  \n",
    "  # Al termine stamo l'accuracy sul test set.\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## PROBLEMA #2\n",
    " \n",
    "Prova a modificare il codice precedente impiegando un Stochastic gradient descent.\n",
    "\n",
    "Quanto tempo impiega ora per terminare l'elaborazione?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante l'elaborazone batch l'algoritmo elabora solo un sottoinsieme di dati alla volta.\n",
    "\n",
    "L'elaborazione è ripetuta, perciò conviene scrivere il codice senza gestire la creazione dei dati direttamente.\n",
    "\n",
    "In TF un placeholder è una variabile che assumera i valori a tempo di esecuzione.\n",
    "\n",
    "Possiamo costruire il grafo delle operazioni senza il bisogno di conoscere i dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  4.  6.]\n"
     ]
    }
   ],
   "source": [
    "# Nel seguente codice creiamo una operazione (y) di moltiplicazione * 2 senza sapere i valori.\n",
    "# Ora la possiamo eseguire all'interno di una sessione. Per valutarla occorre fornire (feed)\n",
    "# i valori per x. \n",
    "# None significa che non poniamo vincoli sulla dimensione.\n",
    "x = tf.placeholder(tf.float32, shape=None)\n",
    "y = x * 2\n",
    "\n",
    "# TF supporta tipi di variabili simili a NumPy (es. float32, float64, int32, int64)\n",
    "# https://docs.scipy.org/doc/numpy-1.13.0/user/basics.types.html\n",
    "\n",
    "with tf.Session() as session:\n",
    "    result = session.run(y, feed_dict={x: [1, 2, 3]})\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "N.B. Fuori dallo scope session non possiamo stampare il valore dei tensori durante l'elaborazione.\n",
    "\n",
    "#### x = tf.placeholder(\"float\", None)\n",
    "#### y = x * 2\n",
    "#### print(x) \n",
    "\n",
    "#### Output: \"Tensor(\"Placeholder_11:0\", dtype=float32)\" \n",
    "\n",
    "Stampa solo il tipo e non il valore di x.\n",
    "In alternativa usare https://www.tensorflow.org/api_docs/python/tf/InteractiveSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.   4.   6.]\n [  8.  10.  12.]]\n"
     ]
    }
   ],
   "source": [
    "# Possiamo dare in input anche strutture piu' complesse indicando il formato dei dati con shape.\n",
    "# Es. un qualsiasi numero di righe, ma il numero di colonne pari a 3\n",
    "x = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "y = x * 2\n",
    "\n",
    "with tf.Session() as session:\n",
    "    x_data = [[1, 2, 3],\n",
    "              [4, 5, 6],]\n",
    "    result = session.run(y, feed_dict={x: x_data})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Una immagine a colori (RGB) in formato raw può avere una rappresentazione matriciale\n",
    "\n",
    "#### image = tf.placeholder(\"uint8\", shape=[None, None, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Una operazione placeholder viene usata per alimentare il grafo.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "\n",
    "  # Il resto è uguale al precedente esempio.\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  logits = tf.matmul(tf_valid_dataset, weights) + biases\n",
    "  valid_prediction = tf.nn.softmax(logits) \n",
    "  logits = tf.matmul(tf_test_dataset, weights) + biases\n",
    "  test_prediction = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\nMinibatch loss at step 0: 15.735717\nMinibatch accuracy: 13.3%\nValidation accuracy: 15.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 500: 1.864795\nMinibatch accuracy: 75.8%\nValidation accuracy: 75.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 1000: 1.584352\nMinibatch accuracy: 73.4%\nValidation accuracy: 76.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 1500: 1.361099\nMinibatch accuracy: 75.8%\nValidation accuracy: 76.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 2000: 1.038256\nMinibatch accuracy: 79.7%\nValidation accuracy: 77.5%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 2500: 0.977711\nMinibatch accuracy: 75.0%\nValidation accuracy: 78.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 3000: 1.129219\nMinibatch accuracy: 78.1%\nValidation accuracy: 78.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 86.1%\n"
     ]
    }
   ],
   "source": [
    "# Se impiego minibatch potenzialmente ho più varianza nell'apprendimento ad ogni ciclo.\n",
    "# Sono costretto ad aumentare gli step.\n",
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "    \n",
    "  for step in range(num_steps):\n",
    "    # Definisco un offset nel trainig set\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    \n",
    "    # Estraggo ilminibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "    # Dizionario {chiave_placeholder : valore, ...}\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    \n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "    \n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L'accuratezza sul minibatch diminuisce mentre quella sulla validation aumenta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## PROBLEMA #3\n",
    "\n",
    "Usando l'help online di TF prova a creare una rete neurale con 1-hidden layer con attivazione RELU e 1024 nodi nascosti.\n",
    "\n",
    "N.B. la funzione tf.nn.relu() restituisce un tensore che calcola la RELU sul tensore di input.\n",
    "\n",
    "L'output ha la stessa dimensione dell'input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodi del hidden layer\n",
    "hidden_nodes= 1024\n",
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Avendo un layer in più ho due coppie <W,B>\n",
    "    # N.B. W1 e B1 hanno dimensioni <#input-feature-vector,#hidden-nodes>, <#hidden-nodes>\n",
    "    # mentre W2 <#hidden-nodes,num_labels>, <num_labels>\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "    biases_1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    weights_2 = tf.Variable(tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "    biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    \n",
    "    logits_1 = tf.matmul(tf_train_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "\n",
    "    # Ora la loss function è definita sullo layer di output\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits \\\n",
    "                          (labels=tf_train_labels, logits=logits_2))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits_2)\n",
    "    # Seguo la stessa pipeline per valid e test set\n",
    "    logits_1 = tf.matmul(tf_valid_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    valid_prediction = tf.nn.softmax(logits_2) \n",
    "    \n",
    "    logits_1 = tf.matmul(tf_test_dataset, weights_1) + biases_1\n",
    "    relu_layer= tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "    test_prediction = tf.nn.softmax(logits_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\nMinibatch loss at step 0: 418.583008\nMinibatch accuracy: 8.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 33.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 500: 16.843128\nMinibatch accuracy: 77.3%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 80.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 1000: 26.599648\nMinibatch accuracy: 78.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 79.5%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 1500: 8.241529\nMinibatch accuracy: 75.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 81.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 2000: 6.842561\nMinibatch accuracy: 85.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 81.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 2500: 4.103213\nMinibatch accuracy: 80.5%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 82.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 3000: 3.539648\nMinibatch accuracy: 80.5%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 81.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 88.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "    \n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    \n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    \n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    \n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "    \n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L'accuratezza sul minibatch diminuisce mentre quella sulla validation aumenta molto più rapidamente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
